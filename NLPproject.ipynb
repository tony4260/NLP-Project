{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNXRm6Utp2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "2e2ac57f-2d95-4996-e35a-829dd37601ac"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn.metrics import classification_report, roc_curve\n",
        "import plotly.plotly as py\n",
        "import plotly.tools as tls\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from string import punctuation\n",
        "import re\n",
        "import sys\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import zipfile\n",
        "import progressbar\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from google.colab import files\n",
        "import pickle\n",
        "from keras.models import load_model, model_from_json\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2236c439f775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/plotly/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_plotly_future_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_chart_studio_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m_chart_studio_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"plotly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/_plotly_future_/__init__.py\u001b[0m in \u001b[0;36m_chart_studio_error\u001b[0;34m(submodule)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mchart_studio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \"\"\".format(\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0msubmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m     )\n",
            "\u001b[0;31mImportError\u001b[0m: \nThe plotly.plotly module is deprecated,\nplease install the chart-studio package and use the\nchart_studio.plotly module instead. \n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EqQ04KAviXO"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohfZfzx3viyC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-AQ6d1svjQw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQ6fnX-vjmL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_p6qIUOvj9Y"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN0pJzJzvkWm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCpKBur1v2V2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C4uU2FPv3D0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2HCKV9DKUE3"
      },
      "source": [
        "#englishWords = set()\n",
        "#with open('words_alpha.txt', 'r') as handle:\n",
        "#    lines = handle.readlines()\n",
        "#    for line in lines:\n",
        "#      word = line.strip()\n",
        "#      englishWords.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA5pI9bGy7VL"
      },
      "source": [
        "with open('contractions.pickle', 'wb') as handle:\n",
        "    pickle.dump(contractions, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_doTxu9BNNsM"
      },
      "source": [
        "with open('wordConv.pickle', 'wb') as handle:\n",
        "    pickle.dump(wordConvDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpofquQZ5TwE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be46fe46-9f1d-4567-f323-f1f6a1d6b79f"
      },
      "source": [
        "#ignoredW = set()\n",
        "#wordConvDict[\"woz\"] = \"was\"\n",
        "#print(len(englishWords))\n",
        "#for key in contractions.keys():\n",
        "#  print(key,\":\",contractions[key])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "370099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnX9qKdrLE1D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e67be48-b2bb-442a-b12e-3d55179d3359"
      },
      "source": [
        "print(len(wordConvDict.keys()))\n",
        "print(len(contractions.keys()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n",
            "191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLBZgybErDcJ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def BuildModel(modelW):\n",
        "  print(\"Restoring model structure\")\n",
        "  json_file = open(\"model.json\", 'r')\n",
        "  temp_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  temp_model = tf.keras.models.model_from_json(temp_model_json)\n",
        "  \n",
        "  if modelW==1:\n",
        "    print(\"Restoring model weights\")\n",
        "    temp_model.load_weights(\"model.h5\")\n",
        "  \n",
        "  print(\"Compiling Model\")\n",
        "  temp_model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return temp_model\n",
        "\n",
        "\n",
        "def vectorizeData(shuffle,x_temp,y_temp):\n",
        "  \n",
        "  if shuffle==1:\n",
        "    print(\"Zipping x and y together for shuffling\")\n",
        "    shuffleTemp = list(zip(x_temp, y_temp))\n",
        "\n",
        "    print(\"Shuffling Sets\")\n",
        "    random.shuffle(shuffleTemp)\n",
        "\n",
        "    print(\"Un-Zipping Sets\")\n",
        "    x_temp, y_temp = zip(*shuffleTemp)\n",
        "  \n",
        "  print(\"Restoring Vectorizer From vectorize.pickle\")\n",
        "  with open('vectorize.pickle', 'rb') as handle:\n",
        "    vec_dict = pickle.load(handle)\n",
        "  vectorizer = CountVectorizer(vocabulary=vec_dict)\n",
        "  \n",
        "  print(\"Transforming Data\")\n",
        "  x_temp = vectorizer.transform(x_temp)\n",
        "  \n",
        "  print(\"Categorizing Labels\")\n",
        "  y_train_ad = list()\n",
        "  for i in range(0,len(y_temp)):\n",
        "    if y_temp[i] == 0:\n",
        "      y_train_ad.append(0)\n",
        "    else:\n",
        "      y_train_ad.append(1)\n",
        "   \n",
        "  y_temp = to_categorical(y_train_ad)\n",
        "  return x_temp,y_temp\n",
        "  \n",
        "def saveTxt(filename,x_in,y_in):\n",
        "  print(\"Saving Processed Data...\")\n",
        "  fd = open(filename,\"w\")\n",
        "  for i in range(0,len(y_in)):\n",
        "    tempStr = str(y_in[i]) + \"\\\"\" + x_in[i] +\"\\n\"\n",
        "    fd.write(tempStr)\n",
        "  fd.close()\n",
        "  print(\"...All Processed Data Saved to File:\",filename)\n",
        "\n",
        "def loadTxt(filename=None):\n",
        "  if filename is None:\n",
        "    print(\"Select .csv File to Read and Process:\")\n",
        "    upload = files.upload()\n",
        "    filename = list(upload.keys())[0]\n",
        "    \n",
        "  print(\"Loading Pre-Processed Data...\")\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  y_temp = list()\n",
        "  x_temp = list()\n",
        "\n",
        "  fd = open(filename,\"r\")\n",
        "\n",
        "  lines = fd.readlines()\n",
        "  prog = 0\n",
        "  bar = progressbar.ProgressBar(maxval=len(lines), \\\n",
        "    widgets=[progressbar.Percentage(), ' ', progressbar.Bar('=', '[', ']'), ' ', progressbar.SimpleProgress()])\n",
        "  bar.start()\n",
        "\n",
        "  for line in lines:\n",
        "    wList = line.split(\"\\\"\")\n",
        "    y_temp.append(int(wList[0]))\n",
        "    x_temp.append(wList[-1].strip())\n",
        "    prog+=1\n",
        "    bar.update(prog)\n",
        "  \n",
        "  fd.close()\n",
        "  bar.finish()\n",
        "  print(\"...All Training Data Loaded\\n\")\n",
        "  return x_temp, y_temp\n",
        "\n",
        "\n",
        "def procCSV(filename=None):\n",
        "  \n",
        "#  with open('contractions.pickle', 'rb') as handle:\n",
        "#    contractions = pickle.load(handle)\n",
        "    \n",
        "#  with open('wordConv.pickle', 'rb') as handle:\n",
        "#    wordConvDict = pickle.load(handle)\n",
        "  \n",
        "#  newPunc = punctuation.replace(\"'\",\"\")\n",
        "  if filename is None:\n",
        "    print(\"Select .csv File to Read and Process:\")\n",
        "    upload = files.upload()\n",
        "    filename = list(upload.keys())[0]\n",
        "    \n",
        "  print(\"Reading and Pre-Processing Data...\")\n",
        "  sys.stdout.flush()\n",
        "  \n",
        "  x_temp = list()\n",
        "  y_temp = list()\n",
        "  fd = open(filename, \"r\", errors='ignore')\n",
        "  lines = fd.readlines()\n",
        "  fd.close()\n",
        "  \n",
        "#  weirdW = {}\n",
        "  \n",
        "  prog = 0\n",
        "  bar = progressbar.ProgressBar(maxval=len(lines), \\\n",
        "    widgets=[progressbar.Percentage(), ' ', progressbar.Bar('=', '[', ']'), ' ', progressbar.SimpleProgress()])\n",
        "  bar.start()\n",
        "  for line in lines:\n",
        "    line = line.lower()\n",
        "    wList = line.split(\"\\\"\")\n",
        "    newL = []\n",
        "    for seg in wList:\n",
        "      if (len(seg) < 5 and \",\" in seg) or (len(seg) == 0) or \"\\n\" in seg:\n",
        "        count = 1\n",
        "      else:\n",
        "        newL.append(seg)\n",
        "    curLab = newL[0].rstrip()\n",
        "    curTweet = ''\n",
        "    for elem in newL[5:]:\n",
        "      tempElem1 = re.sub(r'\\.\\.+',' ',elem)\n",
        "      tempElem1 = re.sub(r'(^|\\s)@[a-z0-9]*?\\s','',tempElem1)\n",
        "      tempElem1 = re.sub(r'&quot;',' ',tempElem1)\n",
        "      tempElem = ''.join(c for c in elem if c not in punctuation)\n",
        "      tempWords = tempElem.split()\n",
        "      for temp in tempWords:\n",
        "        rplW = temp\n",
        "#        if \"'\" in temp and temp not in contractions.keys():\n",
        "#          if \"n'\" in temp[-2:]:\n",
        "#            rplW.replace(\"'\",\"g\")\n",
        "#        elif temp in contractions.keys():\n",
        "#          resultCont = contractions[temp]\n",
        "#          resultContL = resultCont.split(\"/\")\n",
        "#          if len(resultContL) > 1:\n",
        "#            rplW = resultContL[random.randint(0,len(resultContL)-1)]\n",
        "#          else:\n",
        "#            rplW = resultContL[0]\n",
        "#        elif \"'ll\" in rplW:\n",
        "#          rplW.replace(\"'ll\",\" will\")\n",
        "        curTweet += rplW\n",
        "        curTweet += ' '\n",
        "  \n",
        "    curTweet = curTweet.lower()  \n",
        "    curTweet = re.sub(r'__[0-9]/[0-9]','',curTweet)\n",
        "    curTweet = re.sub(r'(^|\\s)rt(\\s|$)',' retweet ',curTweet)\n",
        "    curTweet = re.sub(r'[a-z0-9\\.]*?@[a-z0-9]*?\\.(gov|edu|net|org|com).*?','address',curTweet)\n",
        "    curTweet = re.sub(r'@',' at ',curTweet)\n",
        "    curTweet = re.sub(r'[!?]+',' ',curTweet)\n",
        "    curTweet = re.sub(r'\\.\\.+',' ',curTweet)\n",
        "    curTweet = re.sub(r'\\$\\$+',' money ',curTweet)\n",
        "    curTweet = re.sub(r'[\\'\\.:,;-]+\\s',' ',curTweet)\n",
        "    curTweet = re.sub(r'\\s[,\\']',' ',curTweet)\n",
        "    curTweet = re.sub(r'[\\'\\.!,\\s\\t\\n]$','',curTweet)\n",
        "    curTweet = re.sub(r'^[\\'\\.!,\\s\\t\\n]','',curTweet)\n",
        "    curTweet = re.sub(r'#\\s',' #',curTweet)\n",
        "    curTweet = re.sub(r'\\s+',' ',curTweet)\n",
        "    curTweet = re.sub(r'\\s$','',curTweet)\n",
        "    curTweet = curTweet.strip()\n",
        "    \n",
        "    finalWL = curTweet.split()\n",
        "#    for i in range(0,len(finalWL)):\n",
        "#      if finalWL[i] in wordConvDict.keys():\n",
        "#        finalWL[i] = wordConvDict[finalWL[i]]\n",
        "    \n",
        "    curTweet = ''\n",
        "    for word in finalWL:\n",
        "      curTweet += word\n",
        "      curTweet += ' '\n",
        "\n",
        "#    finalWL = curTweet.split()\n",
        "#    for word in finalWL:\n",
        "#      if word not in englishWords:\n",
        "#        if word in weirdW.keys():\n",
        "#          weirdW[word][0]+=1\n",
        "#        else:\n",
        "#          weirdW[word] = [1,curTweet] \n",
        "#    curTweet = ''\n",
        "#    for word in finalWL:\n",
        "#      curTweet += word\n",
        "#      curTweet += ' '\n",
        "  \n",
        "    if int(curLab) != 2:\n",
        "      y_temp.append(int(curLab))\n",
        "      x_temp.append(curTweet)  \n",
        "    \n",
        "    prog+=1\n",
        "    bar.update(prog)\n",
        "    \n",
        "  bar.finish()\n",
        "  print(\"...All Data Finished\")\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#  deleteList = []\n",
        "#  for key in weirdW.keys():\n",
        "#    if weirdW[key][0] < 50 or key in ignoredW:\n",
        "#      deleteList.append(key)\n",
        "#  for wDel in deleteList:  \n",
        "#    del weirdW[wDel]\n",
        "#  unkCount = len(weirdW.keys())\n",
        "#  leftCount = unkCount\n",
        "#  print(len(wordConvDict)) \n",
        "#  print(\"There were\",unkCount,\"Unknown Words\")\n",
        "#  updateDict = input(\"Would You Like to Update Word Dictionary? (Y-Yes or N-No)\\n\")\n",
        "#  if \"Y\" in updateDict:\n",
        "#    print(\"Input String to Convert Word to in the Future or Leave the Field Blank to Skip\")\n",
        "#    for key in weirdW.keys():\n",
        "#      if leftCount % 5 == 0:\n",
        "#        print(unkCount,\"Words Unknown\")\n",
        "#        print(leftCount,\"Words Left\")\n",
        "#      print(\"Word:           \",key,\"\\n#Times Appeared:\",weirdW[key][0],\"\\nExample:        \",weirdW[key][1])\n",
        "#      change = input()\n",
        "#      if change:\n",
        "#        wordConvDict[key] = change\n",
        "#        unkCount-=1\n",
        "#      else:\n",
        "#        ignoredW.add(key)\n",
        "#      leftCount-=1\n",
        "#    print(len(wordConvDict))  \n",
        "#  else:\n",
        "#    print(\"No Updates to Dictionary Made\")\n",
        "    \n",
        "  return x_temp, y_temp\n",
        "  \n",
        "#Returns vectorized x_data, categorized y_data, and weighted, compiled model in x_data, y_data, model format\n",
        "def setup(csv=0, output_proc=None, txt=0, input_file=None, only_proc=0, shuffle=0, zipUp=0, makeModel=0, modelW=0):\n",
        "  x_temp = None\n",
        "  y_temp = None\n",
        "  \n",
        "  if zipUp == 0:\n",
        "    print(\"Select T14.zip File to restore model and vocabulary:\")\n",
        "    upload = files.upload()\n",
        "  \n",
        "  #Unzip file to current directory\n",
        "  zip_ref = zipfile.ZipFile('T14.zip', 'r')\n",
        "  zip_ref.extractall('./')\n",
        "  zip_ref.close()\n",
        "  \n",
        "  if csv == 1:\n",
        "    x_temp, y_temp = procCSV(input_file)\n",
        "  \n",
        "  if txt == 1:\n",
        "    x_temp, y_temp = loadTxt(input_file)\n",
        "    \n",
        "  if csv == 1 and output_proc is not None:\n",
        "    saveTxt(output_proc,x_temp,y_temp)\n",
        "  \n",
        "  if only_proc == 1:\n",
        "    return x_temp, y_temp\n",
        "  \n",
        "  \n",
        "  \n",
        "  x_temp,y_temp = vectorizeData(shuffle, x_temp, y_temp)\n",
        "  if makeModel == 1:\n",
        "    temp_model = BuildModel(modelW)\n",
        "    return x_temp, y_temp, temp_model\n",
        "  return x_temp, y_temp\n",
        "\n",
        "######################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDV4GItx89Vw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26e048c4-9821-48d1-a05c-b95b3f26cbd4"
      },
      "source": [
        "#################################################################\n",
        "\n",
        "#ONLY RUN IF SAVING PROCESSED ARRAYS \"*_proc.txt\" TO FILES\n",
        "\n",
        "print(\"Saving Processed Data...\")\n",
        "\n",
        "fd = open(\"./training_proc.txt\",\"w\")\n",
        "for i in range(0,len(y_train)):\n",
        "  tempStr = str(y_train[i]) + \"\\\"\" + x_train[i] +\"\\n\"\n",
        "  fd.write(tempStr)\n",
        "fd.close()\n",
        "\n",
        "print(\"...All Processed Data Saved to File: training_proc.txt\")\n",
        "\n",
        "#################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving Processed Data...\n",
            "...All Processed Data Saved to File: training_proc.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39mMwvv09EDZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "58631bf7-9236-4e99-dd81-cb13d178cc0d"
      },
      "source": [
        "#################################################################\n",
        "\n",
        "#ONLY RUN IF LOADING PROCESSED DATA TO ARRAYS FROM \"*_proc.txt\" FILES\n",
        "\n",
        "print(\"Loading Pre-Processed Training Data...\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "y_train = []\n",
        "x_train = []\n",
        "\n",
        "fd = open(\"./training_proc.txt\",\"r\")\n",
        "\n",
        "lines = fd.readlines()\n",
        "prog = 0\n",
        "bar = progressbar.ProgressBar(maxval=len(lines), \\\n",
        "    widgets=[progressbar.Percentage(), ' ', progressbar.Bar('=', '[', ']'), ' ', progressbar.SimpleProgress()])\n",
        "bar.start()\n",
        "\n",
        "for line in lines:\n",
        "  wList = line.split(\"\\\"\")\n",
        "  y_train.append(int(wList[0]))\n",
        "  x_train.append(wList[-1].strip())\n",
        "  prog+=1\n",
        "  bar.update(prog)\n",
        "  \n",
        "fd.close()\n",
        "bar.finish()\n",
        "print(\"...All Training Data Loaded\\n\")\n",
        "\n",
        "#################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Pre-Processed Training Data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% [=======================================================] 251197 of 251197\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...All Training Data Loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqqhT-dOEejv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c8b41c37-7b73-4ebf-c98b-8e6187b39a45"
      },
      "source": [
        "x_train, y_train = setup(csv=1,input_file='NLP-classification-training-data.csv',zipUp=1, only_proc=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading and Pre-Processing Data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% [=======================================================] 251197 of 251197\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...All Data Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eCIBiaQ9zjI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b982c1ff-0032-48cf-ad2c-add64606a09c"
      },
      "source": [
        "print(\"Zipping training x and y together\")\n",
        "shuffleTrain = list(zip(x_train, y_train))\n",
        "\n",
        "print(\"Shuffling Training Set\")\n",
        "random.shuffle(shuffleTrain)\n",
        "\n",
        "print(\"Done Shuffling\")\n",
        "x_train, y_train = zip(*shuffleTrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zipping training x and y together\n",
            "Shuffling Training Set\n",
            "Done Shuffling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMUp3zZy98IQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7888ddf-fd54-4c85-9b67-eb1f04d67ae2"
      },
      "source": [
        "#Convert to tokens\n",
        "vectorizer = CountVectorizer(min_df=25)#min_df=7,lowercase=False)#min_df=32, lowercase=False)\n",
        "\n",
        "print(\"Fitting Training Data to Vectorizer\")\n",
        "\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "print(\"Number of Unique Words Accepted:\",len(vectorizer.vocabulary_.keys()))\n",
        "\n",
        "\n",
        "#print(\"Transforming Training Data\")\n",
        "#x_train_v = vectorizer.transform(x_train)\n",
        "\n",
        "#print(\"All Data Transformed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting Training Data to Vectorizer\n",
            "Number of Unique Words Accepted: 6146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPwR4T8Ou8hu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "878d87f9-81b3-4063-9052-d838c8c0ba90"
      },
      "source": [
        "from time import sleep\n",
        "x_train_v = []\n",
        "wordsList = list(vectorizer.vocabulary_.keys())\n",
        "wordsSet = set(wordsList)\n",
        "\n",
        "prog = 0\n",
        "emptyCount = 0\n",
        "bar = progressbar.ProgressBar(maxval=len(x_train), \\\n",
        "    widgets=[progressbar.Percentage(), ' ', progressbar.Bar('=', '[', ']'), ' ', progressbar.SimpleProgress()])\n",
        "bar.start()\n",
        "\n",
        "for tweet in x_train:\n",
        "  tempSent = []\n",
        "  tweetWords = tweet.split()\n",
        "  for word in tweetWords:\n",
        "    if word in wordsSet:\n",
        "      tempSent.append(wordsList.index(word))\n",
        "  if len(tempSent) == 0:\n",
        "    emptyCount += 1\n",
        " \n",
        "  x_train_v.append(tempSent)\n",
        "      \n",
        "  prog+=1\n",
        "  bar.update(prog)\n",
        "bar.finish()\n",
        "  \n",
        "print(len(x_train_v))\n",
        "print(emptyCount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% [=======================================================] 251197 of 251197\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "251197\n",
            "1652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLp2Xn9xCpW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aec69db6-1b45-430e-ab77-634ca4a576b5"
      },
      "source": [
        "print('Maximum review length: {}'.format(\n",
        "len(max((x_train_v),key=len))))\n",
        "print('Minimum review length: {}'.format(\n",
        "len(min((x_train_v),key=len))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 32\n",
            "Minimum review length: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJFbec7Y1Bb4"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "max_words = 30\n",
        "x_train_v = sequence.pad_sequences(x_train_v,maxlen=max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprv8-3UIzEf"
      },
      "source": [
        "#Save Vectorizer Dict\n",
        "with open('vectorize.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer.vocabulary_, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJQ9IfcM-lQZ"
      },
      "source": [
        "#Counts for graph; Given Seperate Cell in case graph cell ran more than once\n",
        "\n",
        "trainNeg = 0\n",
        "trainPos = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUQhwv8Q-ny5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "02c5a7fc-d9ce-40b4-979a-8b82b524fd1f"
      },
      "source": [
        "#Graph Data Sets\n",
        "\n",
        "#Graph Data to Graph number of tweets per set and category\n",
        "bc_fig = plt.figure()\n",
        "ax = bc_fig.add_subplot(111)\n",
        "\n",
        "N = 2\n",
        "\n",
        "y_train_ad = []\n",
        "if trainNeg == 0:\n",
        "  for i in range(0,len(y_train)):\n",
        "    if y_train[i] == 0:\n",
        "      trainNeg+=1\n",
        "      y_train_ad.append(0)\n",
        "    else:\n",
        "      trainPos+=1\n",
        "      y_train_ad.append(1)\n",
        "\n",
        "s1Vals = (trainNeg,trainPos)\n",
        "\n",
        "ind = np.arange(N)\n",
        "width = 0.4\n",
        "\n",
        "p1 = ax.bar(ind,s1Vals,width,color=(0.2588,0.4433,1.0))\n",
        "\n",
        "ax.set_ylabel('# of Tweets')\n",
        "ax.set_xlabel('Category')\n",
        "ax.set_title('Training Tweets by Category')\n",
        "\n",
        "ax.set_xticks(ind)\n",
        "ax.set_xticklabels(('Negative','Positive'))\n",
        "\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuKcvshwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmckCY2h_K5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1da753d3-68e6-4007-c26c-16572ddd230c"
      },
      "source": [
        "#Categorize labels\n",
        "# 86% 83%   min df 7 batch size 32\n",
        "y_train_c = to_categorical(y_train_ad)\n",
        "\n",
        "vocab_size = x_train_v.shape[1]\n",
        "batch_size = 32\n",
        "\n",
        "#Build Model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(512, input_shape=(vocab_size,), activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "model.add(keras.layers.Dense(2, activation=tf.nn.softmax))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#Compile Model\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 512)               7865344   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 8,129,026\n",
            "Trainable params: 8,129,026\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad1zEVIObYkk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d32d6f93-1b14-4301-bbf8-216c1513b588"
      },
      "source": [
        "#Categorize labels\n",
        "#     min df \n",
        "#y_train_c = to_categorical(y_train_ad)\n",
        "\n",
        "vocab_size = len(wordsList)\n",
        "\n",
        "embed_dim = 512\n",
        "lstm_out = 150\n",
        "batch_size = 32\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, embed_dim,input_length=max_words))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "model.add(keras.layers.Bidirectional(keras.layers.LSTM(lstm_out,return_sequences=True)))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "model.add(keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
        "model.add(keras.layers.LSTM(int(lstm_out/2)))\n",
        "model.add(keras.layers.Dropout(0.15))\n",
        "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 30, 512)           3146752   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 30, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 30, 300)           795600    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 30, 300)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 30, 128)           115328    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 15, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 75)                61200     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 75)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 4,118,956\n",
            "Trainable params: 4,118,956\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQWyqYv3f32i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "72558d7b-9417-4652-c82e-a92051bdda9d"
      },
      "source": [
        "#Categorize labels\n",
        "#81 and 80 min_df 25\n",
        "#y_train_c = to_categorical(y_train_ad)\n",
        "\n",
        "vocab_size = len(wordsList)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 200\n",
        "batch_size = 64\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, embed_dim,input_length=max_words))\n",
        "model.add(keras.layers.Bidirectional(keras.layers.LSTM(lstm_out,activation='relu',return_sequences=True)))\n",
        "model.add(keras.layers.Dropout(0.02))\n",
        "model.add(keras.layers.Bidirectional(keras.layers.LSTM(lstm_out,activation='relu',return_sequences=True)))\n",
        "model.add(keras.layers.Dropout(0.01))\n",
        "model.add(keras.layers.LSTM(int(lstm_out/2),activation='relu',return_sequences=True))\n",
        "model.add(keras.layers.LSTM(int(lstm_out/4)))\n",
        "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 30, 128)           778112    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 30, 400)           526400    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 30, 400)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 30, 400)           961600    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 30, 400)           0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 30, 100)           200400    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 50)                30200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 2,496,763\n",
            "Trainable params: 2,496,763\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cdX4y8_UrDw"
      },
      "source": [
        "#SUUUUUUUUUUUUUUUUUUUCKS\n",
        "\n",
        "\n",
        "#Categorize labels\n",
        "#80% and 83% min_df 20\n",
        "y_train_c = to_categorical(y_train_ad)\n",
        "\n",
        "vocab_size = len(wordsList)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 200\n",
        "batch_size = 64\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, embed_dim,input_length=max_words))\n",
        "model.add(keras.layers.LSTM(lstm_out,activation='relu',return_sequences=True))\n",
        "model.add(keras.layers.Dropout(0.015))\n",
        "model.add(keras.layers.LSTM(lstm_out,activation='relu',return_sequences=True))\n",
        "model.add(keras.layers.Dropout(0.005))\n",
        "model.add(keras.layers.LSTM(int(lstm_out/2),activation='relu',return_sequences=True))\n",
        "model.add(keras.layers.LSTM(int(lstm_out/4)))\n",
        "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR-EPySI_o-I"
      },
      "source": [
        "x_test_v = x_train_v[240000:]\n",
        "y_test_c = y_train_c[240000:]\n",
        "x_train_v = x_train_v[:240000]\n",
        "y_train_c = y_train_c[:240000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXuu4Eam_c9Y"
      },
      "source": [
        "#ONLY RUN IF LOADING WEIGHTS FROM 'model.h5' FILE\n",
        "\n",
        "#Load Model\n",
        "model.load_weights(\"./model.h5\")\n",
        "print(\"Successfully Loaded model from disk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjuNaiV5_d7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "12897816-6c20-4467-ecb2-a3aa1644ea2d"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "call_backs = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
        "            tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only=True,\n",
        "                            save_weights_only=False)]\n",
        "\n",
        "\n",
        "#Train Model\n",
        "history = model.fit(x_train_v,\n",
        "                    y_train_ad,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=50,\n",
        "                    callbacks=call_backs,\n",
        "                    validation_split=0.15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 213517 samples, validate on 37680 samples\n",
            "Epoch 1/50\n",
            "213517/213517 [==============================] - 944s 4ms/sample - loss: 0.4629 - acc: 0.7817 - val_loss: 0.4316 - val_acc: 0.8003\n",
            "Epoch 2/50\n",
            "   896/213517 [..............................] - ETA: 14:14 - loss: 0.4032 - acc: 0.8170"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpbZ_RneNxhf"
      },
      "source": [
        "########################################\n",
        "#Saves model layout to json file\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_8ZPHMN9xEj"
      },
      "source": [
        "x_test, y_test = setup(csv=1,input_file='testing.csv',zipUp=1, only_proc=1)\n",
        "#x_test_v = vectorizer.transform(x_test)\n",
        "#x_test_v = vectorizer.transform(x_test)\n",
        "x_test_v=[]\n",
        "\n",
        "prog = 0\n",
        "emptyCount = 0\n",
        "bar = progressbar.ProgressBar(maxval=len(x_test), \\\n",
        "    widgets=[progressbar.Percentage(), ' ', progressbar.Bar('=', '[', ']'), ' ', progressbar.SimpleProgress()])\n",
        "bar.start()\n",
        "\n",
        "for tweet in x_test:\n",
        "  tempSent = []\n",
        "  tweetWords = tweet.split()\n",
        "  for word in tweetWords:\n",
        "    if word in wordsSet:\n",
        "      tempSent.append(wordsList.index(word))\n",
        "  if len(tempSent) == 0:\n",
        "    emptyCount += 1\n",
        " \n",
        "  x_test_v.append(tempSent)\n",
        "      \n",
        "  prog+=1\n",
        "  bar.update(prog)\n",
        "bar.finish()\n",
        "\n",
        "x_test_v = sequence.pad_sequences(x_test_v,maxlen=max_words)\n",
        "\n",
        "\n",
        "y_test_ad = []\n",
        "\n",
        "for i in range(0,len(y_test)):\n",
        "  if y_test[i] == 0:\n",
        "    y_test_ad.append(0)\n",
        "  else:\n",
        "    y_test_ad.append(1)\n",
        "    \n",
        "#y_test_c = to_categorical(y_test_ad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3-mpWyKBWTY"
      },
      "source": [
        "import numpy as np \n",
        "from sklearn.metrics import classification_report, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Downscaled values: 0-Negative 1-Positive\")\n",
        "\n",
        "#Test Model Predictions and output results\n",
        "predictions = model.predict(x_test_v)\n",
        "\n",
        "#Convert Prediction Probabilities to prediction value\n",
        "predR = np.round(predictions)\n",
        "predRI = predR.astype(int)\n",
        "\n",
        "#predInd = []\n",
        "#for lis in predRI:\n",
        "#  predInd.append(lis.tolist().index(1))\n",
        "#y_test = []\n",
        "#for lis in y_test_c:\n",
        "#  y_test.append(lis.tolist().index(1))\n",
        "\n",
        "\n",
        "#Create and output report\n",
        "report = classification_report(y_test_ad, predRI)\n",
        "print(report)\n",
        "\n",
        "#Create ROC_Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test_ad,predRI,pos_label=1)\n",
        "\n",
        "#Plot Curve\n",
        "plt.plot(fpr,tpr, '-ro')\n",
        "plt.xlabel('FPR')\n",
        "plt.ylabel('TPR')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}